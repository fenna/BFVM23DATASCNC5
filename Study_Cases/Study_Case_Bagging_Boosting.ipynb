{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571c496e",
   "metadata": {},
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c933ef68",
   "metadata": {},
   "source": [
    "In this small notebook, we are going to take a look at three differen ensemble learning  techniques:\n",
    "\n",
    "1. **Bagging:** Bagging is mostly used to *reduce the variance* in a model. A simple example of bagging is the Random Forest algorithm.\n",
    "\n",
    "2. **Boosting:** Boosting is mostly used to *reduce the bias* in a model. Examples of boosting algorithms are Ada-Boost and Gradient Boost\n",
    "3. **Stacking:** Stacking is mostly used to *increase the prediction accuracy* of a model. For implementing stacking you can use [the mlextend library provided by scikit learn](https://rasbt.github.io/mlxtend/).\n",
    "\n",
    "This notebook is based on [this blog on medium.com](https://medium.com/@saugata.paul1010/ensemble-learning-bagging-boosting-stacking-and-cascading-classifiers-in-machine-learning-9c66cb271674). We have updated the python code and added more concise (and clear) description of the techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9391ebe",
   "metadata": {},
   "source": [
    "# Bagging classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79781af9",
   "metadata": {},
   "source": [
    "Bagging is a technique that involves creating multiple subsets of the training data through random sampling with replacement (bootstrap sampling). Each subset is used to train a separate base model, typically of the same type (e.g., decision trees). These base models are trained independently and in parallel. During the prediction phase, the outputs of all base models are combined, usually by averaging (for regression) or voting (for classification), to obtain the final prediction.\n",
    "\n",
    "The key idea behind bagging is to reduce the *variance of individual models* by introducing randomness in the training process. Since each base model is trained on a slightly different subset of the data, they tend to have different strengths and weaknesses. Combining their predictions helps to reduce the impact of individual model errors and improves the overall predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac4bb1",
   "metadata": {},
   "source": [
    "<img src=\"../Images/more-bagging.jpeg\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca677ab",
   "metadata": {},
   "source": [
    "Have a look at [the documentation of `sklearn.ensemble.BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) to see what is actually happening. Also, read the [BaggingClassifier user guide](https://scikit-learn.org/stable/modules/ensemble.html#bagging) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X, y = data['data'], data['target']\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af569d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf_array = [ RandomForestClassifier(n_estimators=10, random_state=RANDOM_SEED),\n",
    "    ExtraTreesClassifier(n_estimators=5, random_state=RANDOM_SEED),\n",
    "    KNeighborsClassifier(n_neighbors=2),\n",
    "    SVC(C=10000.0, kernel='rbf', random_state=RANDOM_SEED),\n",
    "    RidgeClassifier(alpha=0.1, random_state=RANDOM_SEED),\n",
    "    LogisticRegression(C=20000, penalty='l2', random_state=RANDOM_SEED),\n",
    "    DecisionTreeClassifier(criterion='gini', max_depth=2, random_state=RANDOM_SEED),\n",
    "    AdaBoostClassifier(n_estimators=5,learning_rate=0.001)]\n",
    "\n",
    "labels = [clf.__class__.__name__ for clf in clf_array]\n",
    "\n",
    "normal_accuracy = []\n",
    "normal_std = []\n",
    "bagging_accuracy = []\n",
    "bagging_std = []\n",
    "\n",
    "for clf in clf_array:\n",
    "    cv_scores = cross_val_score(clf, X, y, cv=3, n_jobs=-1)\n",
    "    bagging_clf = BaggingClassifier(clf, max_samples=0.4, max_features=3, random_state=RANDOM_SEED)\n",
    "    bagging_scores = cross_val_score(bagging_clf, X, y, cv=3, n_jobs=-1)\n",
    "    \n",
    "    n_acc = np.round(cv_scores.mean(), 4)\n",
    "    n_std = np.round(cv_scores.std(), 4)\n",
    "    \n",
    "    b_acc = np.round(bagging_scores.mean(), 4)\n",
    "    b_std = np.round(bagging_scores.std(), 4)\n",
    "    \n",
    "    normal_accuracy.append(n_acc)\n",
    "    normal_std.append(n_std)\n",
    "    \n",
    "    bagging_accuracy.append(b_acc)\n",
    "    bagging_std.append(b_std)\n",
    "    \n",
    "    print(f'Accuracy: {n_acc} (+/- {n_std}) [Normal {clf.__class__.__name__}]')\n",
    "    print(f'Accuracy: {b_acc} (+/- {b_std}) [Bagging {clf.__class__.__name__}]')\n",
    "    print ('')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df3a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "n_groups = 8\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "opacity = .7\n",
    "error_config = {'ecolor': '0.3'}\n",
    "normal_clf = ax.bar(index, normal_accuracy, bar_width, alpha=opacity, color='g', yerr=normal_std, error_kw=error_config, label='Normal Classifier')\n",
    "bagging_clf = ax.bar(index + bar_width, bagging_accuracy, bar_width, alpha=opacity, color='c', yerr=bagging_std, error_kw=error_config, label='Bagging Classifier')\n",
    "\n",
    "ax.set_xlabel('Classifiers')\n",
    "ax.set_ylabel('Accuracy scores with variance')\n",
    "ax.set_title('Difference between normal and bagged classifier')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels((labels))\n",
    "ax.legend()#fig.tight_layout()plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8dbff",
   "metadata": {},
   "source": [
    "# Boosting classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f416c98",
   "metadata": {},
   "source": [
    "Boosting is another ensemble learning technique. It aims to improve the performance of a single model by iteratively training weak models and combining them into a strong model.\n",
    "\n",
    "The main idea behind boosting is to sequentially train a series of weak models, where each subsequent model tries to correct the mistakes made by the previous models. The weak models are typically simple models, such as decision trees with limited depth, called \"weak learners.\"\n",
    "\n",
    "The key concept in boosting is the emphasis on misclassified examples during the training process. By iteratively focusing on difficult examples, boosting algorithms can create a strong model that performs well even on complex tasks. The most popular boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e87c25",
   "metadata": {},
   "source": [
    "<img src=\"../Images/more-boosting.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1855f",
   "metadata": {},
   "source": [
    "Have a look at [the documentation for the `GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) and [the documentation for `AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) to see what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cc1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, \n",
    "\n",
    "ada_boost = AdaBoostClassifier(n_estimators=5)\n",
    "grad_boost = GradientBoostingClassifier(n_estimators=10)\n",
    "boosting_labels = ['Ada Boost', 'Gradient Boost']\n",
    "\n",
    "for clf, label in zip([ada_boost, grad_boost], boosting_labels):\n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(f'Accuracy: {scores.mean():.3f}, Variance: (+/-) {scores.std():.3f} [{label}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b8325",
   "metadata": {},
   "source": [
    "# Stacking classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6665830d",
   "metadata": {},
   "source": [
    "Stacking, also known as stacked generalization, is a technique that involves training multiple models in a hierarchical manner. It consists of two or more levels of models. In the first level, also known as the base level, several different models are trained on the training data. Then, their predictions are used as input features to train a meta-model, often called the \"blender\" or \"meta-learner,\" in the second level.\n",
    "\n",
    "The idea behind stacking is to learn how to combine the predictions of different models using another model. The base models capture different aspects of the data, and the meta-model learns how to weight their predictions to make the final prediction. Stacking allows for more complex relationships and interactions among the models, as the meta-model can learn to make predictions based on the patterns observed in the base models' outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf01c5",
   "metadata": {},
   "source": [
    "<img src=\"../Images/more-stacking.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826326d",
   "metadata": {},
   "source": [
    "Have a look at [the documentation of `StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af11070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "clf_array = [RandomForestClassifier(n_estimators=10, random_state=RANDOM_SEED),\n",
    "    ExtraTreesClassifier(n_estimators=5, random_state=RANDOM_SEED),\n",
    "    KNeighborsClassifier(n_neighbors=2),\n",
    "    SVC(C=10_000.0, kernel='rbf', random_state=RANDOM_SEED),\n",
    "    RidgeClassifier(alpha=0.1, random_state=RANDOM_SEED),\n",
    "    LogisticRegression(C=20_000, penalty='l2', random_state=RANDOM_SEED),\n",
    "    DecisionTreeClassifier(criterion='gini', max_depth=2, random_state=RANDOM_SEED),\n",
    "    AdaBoostClassifier(n_estimators=100),\n",
    "    LogisticRegression(random_state=RANDOM_SEED)]\n",
    "\n",
    "# meta classifiers\n",
    "clf = StackingClassifier(estimators=clf_array)\n",
    "\n",
    "labels = [clf.__class__.__name__ for clf in clf_array]\n",
    "acc_list = []\n",
    "var_list = []\n",
    "\n",
    "for clf, label in zip(clf_array, labels):\n",
    "    cv_scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(f'Accuracy: {cv_scores.mean():0.4f} (+/- {cv_scores.std():0.4f}) [{label}]')\n",
    "    acc_list.append(np.round(cv_scores.mean(),4))\n",
    "    var_list.append(np.round(cv_scores.std(),4))\n",
    "    #print(\"Accuracy: {} (+/- {}) [{}]\".format(np.round(scores.mean(),4), np.round(scores.std(),4), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473da2a",
   "metadata": {},
   "source": [
    "The main difference between bagging and stacking is the way they combine multiple models. *Bagging* combines the predictions of independent models through averaging or voting, while *stacking* trains a meta-model to learn how to combine the predictions of different base models. Bagging focuses on reducing variance, while stacking aims to leverage the strengths of individual models and learn how to best combine them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
